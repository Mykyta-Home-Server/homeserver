# ============================================================================
# Monitoring Stack - Docker Compose Configuration
# ============================================================================
# Grafana Loki logging stack for centralized log aggregation and monitoring
#
# Architecture:
#   Promtail → Loki → Grafana
#   (collect)  (store) (visualize)
#
# Services:
#   - Loki: Log aggregation and storage (like Prometheus for logs)
#   - Promtail: Log collector that scrapes Docker container logs
#   - Grafana: Visualization and dashboards
#
# Access:
#   - Grafana UI: https://monitor.mykyta-ryasny.dev
#   - Loki API: http://loki:3100 (internal only)
#
# Official docs:
#   - Loki: https://grafana.com/docs/loki/latest/
#   - Promtail: https://grafana.com/docs/loki/latest/clients/promtail/
#   - Grafana: https://grafana.com/docs/grafana/latest/
# ============================================================================

services:
  # ==========================================================================
  # Loki - Log aggregation system
  # ==========================================================================
  loki:
    profiles: ["monitoring", "all"]
    image: grafana/loki:3.0.0
    container_name: loki

    restart: unless-stopped

    # Networks
    networks:
      - monitoring  # Internal network for Promtail → Loki communication
      - internal    # Internal services network

    # Ports (internal only, not exposed to host)
    ports:
      - "3100:3100"  # Loki HTTP API

    # Volumes
    volumes:
      # Loki configuration file
      - ../services/monitoring/loki/loki-config.yml:/etc/loki/local-config.yaml:ro

      # Loki data storage (logs, index, chunks)
      # This is where your logs are actually stored
      - ../services/monitoring/loki/data:/loki

    # Command override - Use custom config
    command: -config.file=/etc/loki/local-config.yaml

    # Security
    security_opt:
      - no-new-privileges:true

    # Resource limits (adjust based on your log volume)
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

    # Health check
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

    # Labels
    labels:
      - "com.homeserver.group=monitoring"
      - "com.homeserver.description=Log aggregation system"

  # ==========================================================================
  # Promtail - Log collector
  # ==========================================================================
  promtail:
    profiles: ["monitoring", "all"]
    image: grafana/promtail:3.0.0
    container_name: promtail

    restart: unless-stopped

    # Networks
    networks:
      - monitoring  # Connects to Loki

    # Volumes
    volumes:
      # Promtail configuration
      - ../services/monitoring/promtail/promtail-config.yml:/etc/promtail/config.yml:ro

      # Docker socket - Required to read container logs
      # Read-only for security (Promtail only needs to read logs)
      - /var/run/docker.sock:/var/run/docker.sock:ro

      # Docker container log files (alternative to socket, more efficient)
      # Docker stores container logs in JSON format here
      - /var/lib/docker/containers:/var/lib/docker/containers:ro

      # Promtail positions file (tracks which logs have been read)
      # This ensures logs aren't re-sent if Promtail restarts
      - ../services/monitoring/promtail/positions:/tmp/positions

    # Command override - Use custom config
    command: -config.file=/etc/promtail/config.yml

    # Security
    security_opt:
      - no-new-privileges:true

    # Depends on Loki being available
    depends_on:
      loki:
        condition: service_healthy

    # Resource limits
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

    # Labels
    labels:
      - "com.homeserver.group=monitoring"
      - "com.homeserver.description=Log collection agent"

  # ==========================================================================
  # Grafana - Visualization and dashboards
  # ==========================================================================
  grafana:
    profiles: ["monitoring", "all"]
    image: grafana/grafana:10.2.2
    container_name: grafana

    restart: unless-stopped

    # Networks
    networks:
      - monitoring  # Connects to Loki
      - internal    # Connects to internal services
      - proxy       # Connects to Caddy for external access

    # Ports (internal only)
    ports:
      - "3000:3000"  # Grafana web UI

    # Volumes
    volumes:
      # Grafana data (dashboards, users, settings)
      - ../services/monitoring/grafana/data:/var/lib/grafana

      # Grafana configuration
      - ../services/monitoring/grafana/grafana.ini:/etc/grafana/grafana.ini:ro

      # Provisioning - Auto-configure datasources and dashboards
      # This is Infrastructure as Code for Grafana!
      - ../services/monitoring/grafana/provisioning:/etc/grafana/provisioning:ro

    # Environment variables
    environment:
      # Security - Change these!
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin  # TODO: Change this!

      # Server settings
      - GF_SERVER_ROOT_URL=https://monitor.mykyta-ryasny.dev
      - GF_SERVER_DOMAIN=monitor.mykyta-ryasny.dev

      # Disable analytics
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false

      # Enable anonymous access for dashboards (optional)
      # - GF_AUTH_ANONYMOUS_ENABLED=true
      # - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer

    # User - Run as grafana user (non-root)
    user: "472:472"  # Default Grafana user ID

    # Security
    security_opt:
      - no-new-privileges:true

    # Depends on Loki being available
    depends_on:
      loki:
        condition: service_healthy

    # Resource limits
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

    # Health check
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

    # Labels
    labels:
      - "com.homeserver.group=monitoring"
      - "com.homeserver.description=Monitoring and visualization platform"

